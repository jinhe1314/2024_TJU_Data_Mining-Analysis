#!/usr/bin/env python3
"""
TensorFlow Liteæ¨¡å‹è½¬æ¢è„šæœ¬
å°†Kerasæ¨¡å‹è½¬æ¢ä¸ºä¼˜åŒ–çš„TFLiteæ ¼å¼ç”¨äºç§»åŠ¨ç«¯éƒ¨ç½²
"""

import tensorflow as tf
import numpy as np
import os
import json
from pathlib import Path

class ModelConverter:
    def __init__(self, model_path: str, output_dir: str):
        self.model_path = model_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.model = None

    def load_model(self):
        """åŠ è½½åŸå§‹Kerasæ¨¡å‹"""
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_path}")
        try:
            self.model = tf.keras.models.load_model(self.model_path)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   è¾“å…¥æ•°é‡: {len(self.model.inputs)}")
            for i, inp in enumerate(self.model.inputs):
                print(f"   è¾“å…¥ {i+1}: {inp.shape} ({inp.name})")
            print(f"   è¾“å‡ºæ•°é‡: {len(self.model.outputs)}")
            for i, out in enumerate(self.model.outputs):
                print(f"   è¾“å‡º {i+1}: {out.shape} ({out.name})")
            print(f"   å‚æ•°æ•°é‡: {self.model.count_params():,}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def convert_to_tflite(self, quantization: str = "default"):
        """
        è½¬æ¢æ¨¡å‹ä¸ºTFLiteæ ¼å¼

        Args:
            quantization: é‡åŒ–ç­–ç•¥
                - "default": é»˜è®¤ä¼˜åŒ–
                - "float16": 16ä½æµ®ç‚¹é‡åŒ–
                - "int8": 8ä½æ•´æ•°é‡åŒ–
                - "dynamic": åŠ¨æ€èŒƒå›´é‡åŒ–
        """
        print(f"\nğŸ”„ å¼€å§‹è½¬æ¢ä¸ºTensorFlow Liteæ ¼å¼...")
        print(f"   é‡åŒ–ç­–ç•¥: {quantization}")

        # åˆ›å»ºè½¬æ¢å™¨
        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)

        # è®¾ç½®ä¼˜åŒ–ç­–ç•¥
        if quantization == "default":
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
        elif quantization == "float16":
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_types = [tf.float16]
        elif quantization == "int8":
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            converter.target_spec.supported_ops = [
                tf.lite.OpsSet.TFLITE_BUILTINS_INT8
            ]
            converter.inference_input_type = tf.int8
            converter.inference_output_type = tf.int8
        elif quantization == "dynamic":
            converter.optimizations = [tf.lite.Optimize.DEFAULT]

        # è½¬æ¢æ¨¡å‹
        try:
            tflite_model = converter.convert()
            print(f"âœ… TFLiteè½¬æ¢æˆåŠŸ")

            # ä¿å­˜æ¨¡å‹
            if quantization == "default":
                tflite_path = self.output_dir / "glucose_predictor.tflite"
            else:
                tflite_path = self.output_dir / f"glucose_predictor_{quantization}.tflite"

            with open(tflite_path, 'wb') as f:
                f.write(tflite_model)

            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(tflite_path) / (1024 * 1024)
            print(f"ğŸ“ TFLiteæ¨¡å‹å·²ä¿å­˜: {tflite_path}")
            print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

            return tflite_path

        except Exception as e:
            print(f"âŒ TFLiteè½¬æ¢å¤±è´¥: {e}")
            raise

    def create_test_data(self):
        """åˆ›å»ºæµ‹è¯•æ•°æ®ç”¨äºéªŒè¯è½¬æ¢åçš„æ¨¡å‹"""
        print(f"\nğŸ§ª åˆ›å»ºæµ‹è¯•æ•°æ®...")

        # åˆ›å»ºç¤ºä¾‹è¾“å…¥æ•°æ®
        # æ—¶åºæ•°æ®: (batch_size, 10, 51)
        time_series_data = np.random.random((1, 10, 51)).astype(np.float32)
        # é™æ€æ•°æ®: (batch_size, 30)
        static_data = np.random.random((1, 30)).astype(np.float32)

        # ä¿å­˜æµ‹è¯•æ•°æ®
        test_data = {
            "time_series_input": time_series_data.tolist(),
            "static_input": static_data.tolist(),
            "input_shapes": {
                "time_series": [1, 10, 51],
                "static": [1, 30]
            }
        }

        test_data_path = self.output_dir / "test_data.json"
        with open(test_data_path, 'w') as f:
            json.dump(test_data, f, indent=2)

        print(f"ğŸ“ æµ‹è¯•æ•°æ®å·²ä¿å­˜: {test_data_path}")
        return test_data

    def save_model_info(self, tflite_path: Path, quantization: str):
        """ä¿å­˜æ¨¡å‹ä¿¡æ¯"""
        model_info = {
            "original_model": str(self.model_path),
            "tflite_model": str(tflite_path),
            "quantization": quantization,
            "input_info": [
                {
                    "name": inp.name,
                    "shape": inp.shape.as_list(),
                    "dtype": str(inp.dtype)
                }
                for inp in self.model.inputs
            ],
            "output_info": [
                {
                    "name": out.name,
                    "shape": out.shape.as_list(),
                    "dtype": str(out.dtype)
                }
                for out in self.model.outputs
            ],
            "parameters": {
                "total": int(self.model.count_params()),
                "trainable": sum([
                    np.prod(w.shape) + w.shape[0] if len(w.shape) > 1 else w.shape[0]
                    for layer in self.model.layers if layer.get_weights()
                    for w in layer.get_weights()
                ])
            }
        }

        info_path = self.output_dir / "model_info.json"
        with open(info_path, 'w') as f:
            json.dump(model_info, f, indent=2)

        print(f"ğŸ“ æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {info_path}")

    def convert_all_variants(self):
        """åˆ›å»ºæ‰€æœ‰é‡åŒ–å˜ä½“"""
        variants = ["default", "float16", "dynamic"]
        results = []

        for variant in variants:
            try:
                print(f"\n{'='*50}")
                print(f"è½¬æ¢å˜ä½“: {variant}")
                print(f'='*50)

                tflite_path = self.convert_to_tflite(variant)
                self.save_model_info(tflite_path, variant)

                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(tflite_path) / (1024 * 1024)
                results.append({
                    "variant": variant,
                    "path": str(tflite_path),
                    "size_mb": file_size
                })

            except Exception as e:
                print(f"âŒ å˜ä½“ {variant} è½¬æ¢å¤±è´¥: {e}")
                results.append({
                    "variant": variant,
                    "error": str(e)
                })

        # ä¿å­˜è½¬æ¢ç»“æœæ‘˜è¦
        summary_path = self.output_dir / "conversion_summary.json"
        with open(summary_path, 'w') as f:
            json.dump(results, f, indent=2)

        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ TensorFlow Lite æ¨¡å‹è½¬æ¢å·¥å…·")
    print("=" * 50)

    # é…ç½®è·¯å¾„
    model_path = "../models/GCM_model.h5"
    output_dir = "../models"

    try:
        # åˆ›å»ºè½¬æ¢å™¨
        converter = ModelConverter(model_path, output_dir)

        # åŠ è½½åŸå§‹æ¨¡å‹
        converter.load_model()

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = converter.create_test_data()

        # è½¬æ¢æ‰€æœ‰å˜ä½“
        results = converter.convert_all_variants()

        print(f"\nğŸ‰ è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {converter.output_dir}")

        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        print(f"\nğŸ“Š è½¬æ¢ç»“æœæ‘˜è¦:")
        for result in results:
            if "error" in result:
                print(f"   âŒ {result['variant']}: {result['error']}")
            else:
                print(f"   âœ… {result['variant']}: {result['size_mb']:.2f} MB")

    except Exception as e:
        print(f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())