#!/usr/bin/env python3
"""
TensorFlow Liteæ¨¡å‹æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•TFLiteæ¨¡å‹çš„æ¨ç†æ€§èƒ½å’Œå‡†ç¡®æ€§
"""

import tensorflow as tf
import numpy as np
import time
import json
import statistics
from pathlib import Path

class TFLitePerformanceTester:
    def __init__(self, tflite_path: str):
        self.tflite_path = Path(tflite_path)
        self.interpreter = None
        self.input_details = None
        self.output_details = None

    def load_model(self):
        """åŠ è½½TFLiteæ¨¡å‹"""
        print(f"ğŸ“¦ åŠ è½½TFLiteæ¨¡å‹: {self.tflite_path}")
        try:
            self.interpreter = tf.lite.Interpreter(str(self.tflite_path))
            self.interpreter.allocate_tensors()

            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()

            print(f"âœ… TFLiteæ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   è¾“å…¥æ•°é‡: {len(self.input_details)}")
            for i, detail in enumerate(self.input_details):
                print(f"     è¾“å…¥ {i+1}: {detail['shape']} ({detail['dtype']})")

            print(f"   è¾“å‡ºæ•°é‡: {len(self.output_details)}")
            for i, detail in enumerate(self.output_details):
                print(f"     è¾“å‡º {i+1}: {detail['shape']} ({detail['dtype']})")

        except Exception as e:
            print(f"âŒ TFLiteæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def benchmark_inference_speed(self, num_runs: int = 1000):
        """åŸºå‡†æµ‹è¯•æ¨ç†é€Ÿåº¦"""
        print(f"\nâš¡ åŸºå‡†æµ‹è¯•æ¨ç†é€Ÿåº¦ ({num_runs} æ¬¡è¿è¡Œ)...")

        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        time_series_input = np.random.random((1, 10, 51)).astype(np.float32)
        static_input = np.random.random((1, 30)).astype(np.float32)

        # è®¾ç½®è¾“å…¥
        self.interpreter.set_tensor(self.input_details[0]['index'], time_series_input)
        self.interpreter.set_tensor(self.input_details[1]['index'], static_input)

        # é¢„çƒ­
        for _ in range(10):
            self.interpreter.invoke()

        # åŸºå‡†æµ‹è¯•
        inference_times = []
        for i in range(num_runs):
            start_time = time.perf_counter()
            self.interpreter.invoke()
            end_time = time.perf_counter()

            inference_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            inference_times.append(inference_time)

            if (i + 1) % 100 == 0:
                print(f"   è¿›åº¦: {i+1}/{num_runs}")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_time = statistics.mean(inference_times)
        min_time = min(inference_times)
        max_time = max(inference_times)
        p95_time = np.percentile(inference_times, 95)
        p99_time = np.percentile(inference_times, 99)

        performance_stats = {
            "avg_inference_time_ms": avg_time,
            "min_inference_time_ms": min_time,
            "max_inference_time_ms": max_time,
            "p95_inference_time_ms": p95_time,
            "p99_inference_time_ms": p99_time,
            "total_runs": num_runs,
            "throughput_qps": 1000 / (avg_time / 1000)  # QPS = 1000ms / avg_time_ms
        }

        print(f"ğŸ“Š æ¨ç†æ€§èƒ½ç»Ÿè®¡:")
        print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f} ms")
        print(f"   æœ€å¿«æ—¶é—´: {min_time:.2f} ms")
        print(f"   æœ€æ…¢æ—¶é—´: {max_time:.2f} ms")
        print(f"   P95æ—¶é—´: {p95_time:.2f} ms")
        print(f"   P99æ—¶é—´: {p99_time:.2f} ms")
        print(f"   ååé‡: {performance_stats['throughput_qps']:.1f} QPS")

        return performance_stats

    def test_batch_performance(self, batch_sizes: list = [1, 4, 8, 16]):
        """æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°çš„æ€§èƒ½"""
        print(f"\nğŸ“Š æµ‹è¯•æ‰¹é‡æ€§èƒ½...")

        batch_results = []
        for batch_size in batch_sizes:
            print(f"   æ‰¹é‡å¤§å°: {batch_size}")

            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            time_series_input = np.random.random((batch_size, 10, 51)).astype(np.float32)
            static_input = np.random.random((batch_size, 30)).astype(np.float32)

            # è°ƒæ•´è¾“å…¥å¼ é‡
            self.interpreter.resize_tensor_input(
                self.input_details[0]['index'],
                (batch_size, 10, 51)
            )
            self.interpreter.resize_tensor_input(
                self.input_details[1]['index'],
                (batch_size, 30)
            )

            # é‡æ–°åˆ†é…å¼ é‡
            self.interpreter.allocate_tensors()

            # æ›´æ–°è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = self.interpreter.get_input_details()
            output_details = self.interpreter.get_output_details()

            # è®¾ç½®è¾“å…¥
            self.interpreter.set_tensor(input_details[0]['index'], time_series_input)
            self.interpreter.set_tensor(input_details[1]['index'], static_input)

            # é¢„çƒ­
            for _ in range(5):
                self.interpreter.invoke()

            # åŸºå‡†æµ‹è¯•
            inference_times = []
            for _ in range(50):
                start_time = time.perf_counter()
                self.interpreter.invoke()
                end_time = time.perf_counter()

                inference_times.append((end_time - start_time) * 1000)

            avg_time = statistics.mean(inference_times)
            throughput = batch_size / (avg_time / 1000)

            batch_results.append({
                "batch_size": batch_size,
                "avg_time_ms": avg_time,
                "throughput_qps": throughput,
                "time_per_sample_ms": avg_time / batch_size
            })

            print(f"     å¹³å‡æ—¶é—´: {avg_time:.2f} ms, ååé‡: {throughput:.1f} QPS")

        return batch_results

    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print(f"\nğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨...")

        # è·å–åŸºç¡€å†…å­˜ä½¿ç”¨
        import psutil
        process = psutil.Process()
        base_memory = process.memory_info().rss / (1024 * 1024)  # MB

        # åŠ è½½æ¨¡å‹å
        model_memory = process.memory_info().rss / (1024 * 1024)
        model_overhead = model_memory - base_memory

        # ä¼°ç®—æ¨ç†æ—¶çš„å†…å­˜ä½¿ç”¨
        time_series_input = np.random.random((1, 10, 51)).astype(np.float32)
        static_input = np.random.random((1, 30)).astype(np.float32)

        self.interpreter.set_tensor(self.input_details[0]['index'], time_series_input)
        self.interpreter.set_tensor(self.input_details[1]['index'], static_input)

        inference_memory = process.memory_info().rss / (1024 * 1024)
        inference_overhead = inference_memory - model_memory

        memory_stats = {
            "base_memory_mb": base_memory,
            "model_memory_mb": model_memory,
            "model_overhead_mb": model_overhead,
            "inference_memory_mb": inference_memory,
            "inference_overhead_mb": inference_overhead,
            "total_model_size_mb": self.tflite_path.stat().st_size / (1024 * 1024)
        }

        print(f"ğŸ“Š å†…å­˜ä½¿ç”¨ç»Ÿè®¡:")
        print(f"   åŸºç¡€å†…å­˜: {base_memory:.2f} MB")
        print(f"   æ¨¡å‹å†…å­˜: {model_memory:.2f} MB (+{model_overhead:.2f} MB)")
        print(f"   æ¨ç†å†…å­˜: {inference_memory:.2f} MB (+{inference_overhead:.2f} MB)")
        print(f"   æ¨¡å‹æ–‡ä»¶: {memory_stats['total_model_size_mb']:.2f} MB")

        return memory_stats

    def test_accuracy(self, original_model_path: str = None):
        """æµ‹è¯•æ¨¡å‹å‡†ç¡®æ€§ï¼ˆå¦‚æœæœ‰åŸå§‹æ¨¡å‹ï¼‰"""
        print(f"\nğŸ¯ æµ‹è¯•æ¨¡å‹å‡†ç¡®æ€§...")

        # è¿™é‡Œæˆ‘ä»¬å¯ä»¥æµ‹è¯•TFLiteæ¨¡å‹çš„è¾“å‡ºä¸€è‡´æ€§
        # è¿è¡Œå¤šæ¬¡æ¨ç†æ£€æŸ¥è¾“å‡ºç¨³å®šæ€§
        num_tests = 10
        outputs = []

        for i in range(num_tests):
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            time_series_input = np.random.random((1, 10, 51)).astype(np.float32)
            static_input = np.random.random((1, 30)).astype(np.float32)

            # è®¾ç½®è¾“å…¥å¹¶è¿è¡Œ
            self.interpreter.set_tensor(self.input_details[0]['index'], time_series_input)
            self.interpreter.set_tensor(self.input_details[1]['index'], static_input)
            self.interpreter.invoke()

            # è·å–è¾“å‡º
            output = self.interpreter.get_tensor(self.output_details[0]['index'])
            outputs.append(output.copy())

        # æ£€æŸ¥è¾“å‡ºä¸€è‡´æ€§
        outputs_array = np.array(outputs)
        mean_output = np.mean(outputs_array, axis=0)
        std_output = np.std(outputs_array, axis=0)

        consistency_stats = {
            "num_tests": num_tests,
            "mean_prediction": mean_output.tolist(),
            "std_deviation": std_output.tolist(),
            "max_deviation": np.max(np.std(outputs_array, axis=0)),
            "prediction_range": (np.min(outputs_array).tolist(), np.max(outputs_array).tolist())
        }

        print(f"ğŸ“Š é¢„æµ‹ä¸€è‡´æ€§ç»Ÿè®¡:")
        print(f"   æµ‹è¯•æ¬¡æ•°: {num_tests}")
        print(f"   å¹³å‡é¢„æµ‹: {mean_output}")
        print(f"   æ ‡å‡†å·®: {std_output}")
        print(f"   æœ€å¤§åå·®: {np.max(np.std(outputs_array, axis=0)):.4f}")
        print(f"   é¢„æµ‹èŒƒå›´: {np.min(outputs_array):.2f} - {np.max(outputs_array):.2f}")

        return consistency_stats

    def generate_performance_report(self, performance_stats, memory_stats, consistency_stats):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print(f"\nğŸ“‹ ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")

        report = {
            "model_info": {
                "tflite_file": str(self.tflite_path),
                "file_size_mb": self.tflite_path.stat().st_size / (1024 * 1024),
                "input_shapes": [detail['shape'] for detail in self.input_details],
                "output_shapes": [detail['shape'] for detail in self.output_details]
            },
            "performance": performance_stats,
            "memory": memory_stats,
            "consistency": consistency_stats,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        # ä¿å­˜æŠ¥å‘Š
        report_path = self.tflite_path.parent / "performance_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)

        print(f"ğŸ“ ï¿½ï¿½èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª TensorFlow Lite æ€§èƒ½æµ‹è¯•å·¥å…·")
    print("=" * 50)

    # æŸ¥æ‰¾TFLiteæ¨¡å‹ - ä½¿ç”¨ç»å¯¹è·¯å¾„
    base_dir = Path("/home/gitlab-runner/2024_TJU_Data_Mining-Analysis")
    tflite_paths = list((base_dir / "mobile_deployment" / "models").glob("*.tflite"))
    if not tflite_paths:
        tflite_paths = list((base_dir / "mobile_deployment" / "mobile_deployment" / "src" / "output").glob("*.tflite"))
    if not tflite_paths:
        print("âŒ æœªæ‰¾åˆ°TFLiteæ¨¡å‹æ–‡ä»¶")
        return 1

    tflite_path = tflite_paths[0]
    print(f"ğŸ” æ‰¾åˆ°TFLiteæ¨¡å‹: {tflite_path}")

    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = TFLitePerformanceTester(tflite_path)

        # åŠ è½½æ¨¡å‹
        tester.load_model()

        # æ€§èƒ½æµ‹è¯•
        performance_stats = tester.benchmark_inference_speed()
        batch_results = tester.test_batch_performance()
        memory_stats = tester.test_memory_usage()
        consistency_stats = tester.test_accuracy()

        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_performance_report(
            performance_stats, memory_stats, consistency_stats
        )

        print(f"\nğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Š: {report_path}")

        # æ€»ç»“å…³é”®æŒ‡æ ‡
        print(f"\nğŸ“Š å…³é”®æ€§èƒ½æŒ‡æ ‡:")
        print(f"   âœ… æ¨¡å‹å¤§å°: {report['model_info']['file_size_mb']:.2f} MB")
        print(f"   âœ… æ¨ç†æ—¶é—´: {performance_stats['avg_inference_time_ms']:.2f} ms")
        print(f"   âœ… ååé‡: {performance_stats['throughput_qps']:.1f} QPS")
        print(f"   âœ… å†…å­˜ä½¿ç”¨: {memory_stats['total_model_size_mb'] + memory_stats['model_overhead_mb']:.2f} MB")

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())